{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dd0fe3aef0498fb4dcf1a1bd38fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://ec2-3-85-12-54.compute-1.amazonaws.com:8998/\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1609241788334_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-31-94.ec2.internal:20888/proxy/application_1609241788334_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-26-106.ec2.internal:8042/node/containerlogs/container_1609241788334_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 0 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/12/29 11:45:37 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/12/29 11:45:37 INFO Client: Setting up container launch context for our AM\n",
      "20/12/29 11:45:37 INFO Client: Setting up the launch environment for our AM container\n",
      "20/12/29 11:45:37 INFO Client: Preparing resources for our AM container\n",
      "20/12/29 11:45:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/12/29 11:45:39 INFO Client: Uploading resource file:/mnt/tmp/spark-a4a5e485-25b9-4dd4-9da1-42345f7b3a07/__spark_libs__926474258438202667.zip -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/__spark_libs__926474258438202667.zip\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/livy-api-0.6.0-incubating.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/netty-all-4.1.17.Final.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/livy-rsc-0.6.0-incubating.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/commons-codec-1.9.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/hive-site.xml\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/sparkr.zip\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/pyspark.zip\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/py4j-0.10.7-src.zip\n",
      "20/12/29 11:45:40 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/12/29 11:45:40 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/12/29 11:45:40 INFO Client: Uploading resource file:/mnt/tmp/spark-a4a5e485-25b9-4dd4-9da1-42345f7b3a07/__spark_conf__7538576647198300968.zip -> hdfs://ip-172-31-31-94.ec2.internal:8020/user/livy/.sparkStaging/application_1609241788334_0001/__spark_conf__.zip\n",
      "20/12/29 11:45:40 INFO SecurityManager: Changing view acls to: livy\n",
      "20/12/29 11:45:40 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/12/29 11:45:40 INFO SecurityManager: Changing view acls groups to: \n",
      "20/12/29 11:45:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/12/29 11:45:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/12/29 11:45:41 INFO Client: Submitting application application_1609241788334_0001 to ResourceManager\n",
      "20/12/29 11:45:41 INFO YarnClientImpl: Submitted application application_1609241788334_0001\n",
      "20/12/29 11:45:41 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1609241788334_0001 and attemptId None\n",
      "20/12/29 11:45:42 INFO Client: Application report for application_1609241788334_0001 (state: ACCEPTED)\n",
      "20/12/29 11:45:42 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1609242341215\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-31-94.ec2.internal:20888/proxy/application_1609241788334_0001/\n",
      "\t user: livy\n",
      "20/12/29 11:45:43 INFO Client: Application report for application_1609241788334_0001 (state: ACCEPTED)\n",
      "20/12/29 11:45:44 INFO Client: Application report for application_1609241788334_0001 (state: ACCEPTED)\n",
      "20/12/29 11:45:45 INFO Client: Application report for application_1609241788334_0001 (state: ACCEPTED)\n",
      "20/12/29 11:45:46 INFO Client: Application report for application_1609241788334_0001 (state: ACCEPTED)\n",
      "20/12/29 11:45:46 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-31-94.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-31-94.ec2.internal:20888/proxy/application_1609241788334_0001), /proxy/application_1609241788334_0001\n",
      "20/12/29 11:45:46 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/12/29 11:45:46 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/12/29 11:45:47 INFO Client: Application report for application_1609241788334_0001 (state: RUNNING)\n",
      "20/12/29 11:45:47 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.26.106\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1609242341215\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-31-94.ec2.internal:20888/proxy/application_1609241788334_0001/\n",
      "\t user: livy\n",
      "20/12/29 11:45:47 INFO YarnClientSchedulerBackend: Application application_1609241788334_0001 has started running.\n",
      "20/12/29 11:45:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36361.\n",
      "20/12/29 11:45:47 INFO NettyBlockTransferService: Server created on ip-172-31-31-94.ec2.internal:36361\n",
      "20/12/29 11:45:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/12/29 11:45:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-31-94.ec2.internal, 36361, None)\n",
      "20/12/29 11:45:47 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-31-94.ec2.internal:36361 with 1028.8 MB RAM, BlockManagerId(driver, ip-172-31-31-94.ec2.internal, 36361, None)\n",
      "20/12/29 11:45:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-31-94.ec2.internal, 36361, None)\n",
      "20/12/29 11:45:47 INFO BlockManager: external shuffle service port = 7337\n",
      "20/12/29 11:45:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-31-94.ec2.internal, 36361, None)\n",
      "20/12/29 11:45:47 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/12/29 11:45:48 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1609241788334_0001\n",
      "20/12/29 11:45:48 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/12/29 11:45:48 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/12/29 11:45:48 INFO SparkEntries: Spark context finished initialization in 12752ms\n",
      "20/12/29 11:45:48 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/12/29 11:45:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.26.106:51332) with ID 5\n",
      "20/12/29 11:45:50 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 1)\n",
      "20/12/29 11:45:50 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-26-106.ec2.internal:44267 with 5.4 GB RAM, BlockManagerId(5, ip-172-31-26-106.ec2.internal, 44267, None)\n",
      "20/12/29 11:45:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.18.144:48394) with ID 1\n",
      "20/12/29 11:45:51 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 2)\n",
      "20/12/29 11:45:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.155:52930) with ID 3\n",
      "20/12/29 11:45:51 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)\n",
      "20/12/29 11:45:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.23.133:58094) with ID 2\n",
      "20/12/29 11:45:51 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 4)\n",
      "20/12/29 11:45:51 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-18-144.ec2.internal:40421 with 5.4 GB RAM, BlockManagerId(1, ip-172-31-18-144.ec2.internal, 40421, None)\n",
      "20/12/29 11:45:51 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-155.ec2.internal:45751 with 5.4 GB RAM, BlockManagerId(3, ip-172-31-28-155.ec2.internal, 45751, None)\n",
      "20/12/29 11:45:51 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-23-133.ec2.internal:34003 with 5.4 GB RAM, BlockManagerId(2, ip-172-31-23-133.ec2.internal, 34003, None)\n",
      "20/12/29 11:45:51 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.21.125:52800) with ID 4\n",
      "20/12/29 11:45:51 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 5)\n",
      "20/12/29 11:45:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-21-125.ec2.internal:36937 with 5.4 GB RAM, BlockManagerId(4, ip-172-31-21-125.ec2.internal, 36937, None)\n",
      "20/12/29 11:45:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.19.122:52020) with ID 8\n",
      "20/12/29 11:45:52 INFO ExecutorAllocationManager: New executor 8 has registered (new total is 6)\n",
      "20/12/29 11:45:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.18.43:42882) with ID 7\n",
      "20/12/29 11:45:52 INFO ExecutorAllocationManager: New executor 7 has registered (new total is 7)\n",
      "20/12/29 11:45:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.233:33712) with ID 6\n",
      "20/12/29 11:45:52 INFO ExecutorAllocationManager: New executor 6 has registered (new total is 8)\n",
      "20/12/29 11:45:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-19-122.ec2.internal:37989 with 5.4 GB RAM, BlockManagerId(8, ip-172-31-19-122.ec2.internal, 37989, None)\n",
      "20/12/29 11:45:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-18-43.ec2.internal:35695 with 5.4 GB RAM, BlockManagerId(7, ip-172-31-18-43.ec2.internal, 35695, None)\n",
      "20/12/29 11:45:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-233.ec2.internal:44249 with 5.4 GB RAM, BlockManagerId(6, ip-172-31-28-233.ec2.internal, 44249, None)\n",
      "20/12/29 11:45:55 INFO SparkEntries: Created HiveContext.\n",
      "20/12/29 11:46:11 INFO SparkContext: Starting job: count at <stdin>:11\n",
      "20/12/29 11:46:11 INFO DAGScheduler: Got job 0 (count at <stdin>:11) with 32 output partitions\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
