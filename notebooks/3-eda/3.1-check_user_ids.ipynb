{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Check User ID's\n",
    "\n",
    "##### Description\n",
    "\n",
    "The basic relational structure is controlled by the column user_id, which is present in all tables. The data must be checked to ensure all user_ids in transactions, logs, and train have corresponding entries in the members table.\n",
    "\n",
    "##### Notebook Steps\n",
    "\n",
    "1. Connect Spark\n",
    "1. Input all data sources\n",
    "1. Check for parity\n",
    "1. Remove records with no corresponding member record\n",
    "1. Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"mems-clean\")\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- city: integer (nullable = true)\n",
      " |-- registered_via: integer (nullable = true)\n",
      " |-- registration_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "members = sqlContext.read.format('com.databricks.spark.csv').options(inferschema='true', header='true').load('../../data/2-data_cleaning/2-members.output.csv')\n",
    "\n",
    "members.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- payment_method_id: integer (nullable = true)\n",
      " |-- payment_plan_days: integer (nullable = true)\n",
      " |-- plan_list_price: integer (nullable = true)\n",
      " |-- actual_amount_paid: integer (nullable = true)\n",
      " |-- is_auto_renew: boolean (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- membership_expire_date: string (nullable = true)\n",
      " |-- is_cancel: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = sqlContext.read.format('com.databricks.spark.csv').options(inferschema='true', header='true').load('../../data/2-data_cleaning/2-transactions.output.csv')\n",
    "\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_25: integer (nullable = true)\n",
      " |-- num_50: integer (nullable = true)\n",
      " |-- num_75: integer (nullable = true)\n",
      " |-- num_985: integer (nullable = true)\n",
      " |-- num_100: integer (nullable = true)\n",
      " |-- num_unq: integer (nullable = true)\n",
      " |-- total_secs: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = sqlContext.read.format('com.databricks.spark.csv').options(inferschema='true', header='true').load('../../data/2-data_cleaning/2-logs.output.csv')\n",
    "\n",
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- is_churn: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = sqlContext.read.format('com.databricks.spark.csv').options(inferschema='true', header='true').load('../../data/2-data_cleaning/2-train.output.csv')\n",
    "\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *      \n",
    "from pyspark.sql.functions import *  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command performs an anti join to select user_ids in members but not in transactions. This is completed for each table in reference to the members table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_id_transactions = transactions.join(members, ['user_id'], 'leftanti').select('user_id')\n",
    "no_id_logs = logs.join(members, ['user_id'], 'leftanti').select('user_id')\n",
    "no_id_train = train.join(members, ['user_id'], 'leftanti').select('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then outer join the three data frames to get all user ids not in members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_id = no_id_transactions.join(no_id_logs, on='user_id', how='outer')\n",
    "no_id = no_id.join(no_id_train, on='user_id', how='outer')\n",
    "no_id = no_id.select(\"user_id\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of ID's not in members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121409"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431009"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303156"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = transactions.join(no_id, ['user_id'], 'leftanti')\n",
    "transactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18396362"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18395950"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = logs.join(no_id, ['user_id'], 'leftanti')\n",
    "logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970960"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860967"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.join(no_id, ['user_id'], 'leftanti')\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/3-eda/3.1-members.csv'\n",
    "\n",
    "members.write.format('com.databricks.spark.csv').options(header='true').save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/3-eda/3.1-transactions.csv'\n",
    "\n",
    "transactions.write.format('com.databricks.spark.csv').options(header='true').save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/3-eda/3.1-logs.csv'\n",
    "\n",
    "logs.write.format('com.databricks.spark.csv').options(header='true').save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/3-eda/3.1-train.csv'\n",
    "\n",
    "train.write.format('com.databricks.spark.csv').options(header='true').save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
